import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2
import os

# === CONFIG ===
IMG_SIZE = (128, 128)
BATCH_SIZE = 32

# === PATH SETUP ===
base_wave = r"C:\Users\Hooria\OneDrive - YMCAs in Canada\Desktop\archive (1)\drawings\wave"
base_spiral = r"C:\Users\Hooria\OneDrive - YMCAs in Canada\Desktop\archive (1)\drawings\spiral"

train_dirs = [os.path.join(base_wave, "training"),
              os.path.join(base_spiral, "training")]

test_dirs = [os.path.join(base_wave, "testing"),
             os.path.join(base_spiral, "testing")]

# === LOAD & COMBINE TRAINING DATA ===
train_ds1 = tf.keras.preprocessing.image_dataset_from_directory(
    train_dirs[0],
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='int',
    shuffle=True
)
train_ds2 = tf.keras.preprocessing.image_dataset_from_directory(
    train_dirs[1],
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='int',
    shuffle=True
)

# ✅ Save class names before concatenation
class_names = train_ds1.class_names
print("Class names:", class_names)

# ✅ Combine & shuffle
train_ds = train_ds1.concatenate(train_ds2)
train_ds = train_ds.shuffle(100, reshuffle_each_iteration=True)

# === LOAD & COMBINE TESTING DATA ===
test_ds1 = tf.keras.preprocessing.image_dataset_from_directory(
    test_dirs[0],
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='int',
    shuffle=False
)
test_ds2 = tf.keras.preprocessing.image_dataset_from_directory(
    test_dirs[1],
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='int',
    shuffle=False
)
test_ds = test_ds1.concatenate(test_ds2)

# === DATA AUGMENTATION ===
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
])

# === BUILD MODEL ===
base_model = MobileNetV2(input_shape=IMG_SIZE + (3,),
                         include_top=False,
                         weights='imagenet')
base_model.trainable = False  # Freeze base model

parkinsons_model = models.Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(2)  # 2 output classes: healthy, parkinson
])

parkinsons_model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# === TRAIN MODEL ===
history = parkinsons_model.fit(train_ds, validation_data=test_ds, epochs=10)

# === EVALUATE ===
test_loss, test_acc = parkinsons_model.evaluate(test_ds)
print(f"\n✅ Combined Test Accuracy: {test_acc:.2f}")


# Plotting loss and accuracy curves
history = parkinsons_model.fit(train_ds, validation_data=test_ds, epochs=10)
import matplotlib.pyplot as plt

# Extract metrics from history
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(len(acc))

# Plot
plt.figure(figsize=(12, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

# Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

plt.tight_layout()
plt.show()

# Save the model
parkinsons_model.save("parkinsons_model.h5") 


##Feeding new data into model

from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import numpy as np

# Load the trained CNN model
cnn_model = load_model("parkinsons_model.h5") 

def predict_parkinsons_risk(image_path):
    img = image.load_img(image_path, target_size=(128, 128))  
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0) 
    img_array = img_array / 255.0  #normalize

    prediction = cnn_model.predict(img_array)[0] 

    # Interpreting the prediction
    prob = tf.nn.softmax(prediction).numpy()
    parkinsons_score = prob[1] 
    return classify_risk(parkinsons_score)


##Calculating and returning risk analysis

image_path = r"C:\Users\Tosin Boyede\Desktop\cnn_image_test.png"

risk = predict_parkinsons_risk(image_path)

def classify_risk(risk):
    if risk < 0.33:
        return "Low Risk"
    elif risk < 0.66:
        return "Medium Risk"
    else:
        return "High Risk"


